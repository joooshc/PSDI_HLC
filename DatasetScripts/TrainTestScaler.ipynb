{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, normalize\n",
    "from scipy import stats\n",
    "from MiscScripts.DataCleaning import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRobustScaler(data, tol):\n",
    "    print(\"Initial shape:\", data.shape)\n",
    "    data = MinMaxScaler().fit_transform(data)\n",
    "    data, d = dropLowDistinction(pd.DataFrame(data), tol)\n",
    "    data = RobustScaler().fit_transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.01\n",
    "vers = \"0.7.0\"\n",
    "datasets = [\"logHenry\", \"logS\"]\n",
    "trainTest = [\"Train\", \"Validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSingleValCols(data, cols):\n",
    "    \"\"\" \n",
    "    Function that iterates through columns and deletes columns with only one unique value.\n",
    "    \n",
    "    Parameters:\n",
    "    data: pandas DataFrame\n",
    "    cols: list of strings\n",
    "    \n",
    "    Returns:\n",
    "    data: pandas DataFrame\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        values = data[col].values.astype(float).tolist()\n",
    "        if len(set(values)) == 1:\n",
    "            data.drop(col, axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def cleanData(df, idx, tolPercent):\n",
    "    print(\"\\n\\n\", df.columns[0])\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df = df[pd.to_numeric(df[df.columns[0]], errors='coerce').notnull()]\n",
    "\n",
    "    tolVal = len(df) * tolPercent\n",
    "    data = df.iloc[:, idx:]\n",
    "    print(\"Initial data shape:\", data.shape)\n",
    "    data, dropped = dropNaN_cols(data, data.columns, tolVal)\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    data.dropna(inplace=True)\n",
    "    print(\"Shape after removing NaN columns:\", data.shape)\n",
    "\n",
    "    data = removeSingleValCols(data, data.columns)\n",
    "    df = pd.concat([df.iloc[:, :idx], data], axis=1)\n",
    "    print(\"Shape after removing single value columns:\", data.shape)\n",
    "    print(\"Final dataset shape:\", df.shape)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Train\n",
      "\n",
      "\n",
      " logS\n",
      "Initial data shape: (9944, 211)\n",
      "Shape after removing NaN columns: (9944, 198)\n",
      "Shape after removing single value columns: (9944, 156)\n",
      "Final dataset shape: (9944, 164)\n",
      "\n",
      " logS\n",
      "Initial shape: (9944, 164)\n",
      "Shape after dropping low distinction: (9944, 72)\n",
      "Shape after removing outliers: (9916, 79)\n",
      "Shape after removing NaN: (9916, 79)\n",
      "\n",
      "\n",
      " Train\n",
      "\n",
      "\n",
      " logHenry\n",
      "Initial data shape: (10301, 211)\n",
      "Shape after removing NaN columns: (10297, 198)\n",
      "Shape after removing single value columns: (10297, 156)\n",
      "Final dataset shape: (10301, 164)\n",
      "\n",
      " logHenry\n",
      "Initial shape: (10301, 164)\n",
      "Shape after dropping low distinction: (10301, 71)\n",
      "Shape after removing outliers: (10201, 78)\n",
      "Shape after removing NaN: (10197, 78)\n",
      "\n",
      "\n",
      " Validation\n",
      "\n",
      "\n",
      " logS\n",
      "Initial data shape: (1759, 211)\n",
      "Shape after removing NaN columns: (1759, 198)\n",
      "Shape after removing single value columns: (1759, 155)\n",
      "Final dataset shape: (1759, 163)\n",
      "\n",
      " logS\n",
      "Initial shape: (1759, 163)\n",
      "Shape after dropping low distinction: (1759, 81)\n",
      "Shape after removing outliers: (1754, 88)\n",
      "Shape after removing NaN: (1754, 88)\n",
      "\n",
      "\n",
      " Validation\n",
      "\n",
      "\n",
      " logHenry\n",
      "Initial data shape: (1866, 211)\n",
      "Shape after removing NaN columns: (1852, 198)\n",
      "Shape after removing single value columns: (1852, 154)\n",
      "Final dataset shape: (1866, 162)\n",
      "\n",
      " logHenry\n",
      "Initial shape: (1866, 162)\n",
      "Shape after dropping low distinction: (1866, 80)\n",
      "Shape after removing outliers: (1847, 87)\n",
      "Shape after removing NaN: (1833, 87)\n"
     ]
    }
   ],
   "source": [
    "# hasLogSClean = cleanData(hasLogS, 8, 0.01)\n",
    "# hasHenryClean = cleanData(hasHenry, 8, 0.01)\n",
    "\n",
    "datasets = [\"logS\", \"logHenry\"]\n",
    "vers = \"0.7.0\"\n",
    "colDict = {}\n",
    "\n",
    "for tt in trainTest:\n",
    "    for dset in datasets:\n",
    "        print(\"\\n\\n\", tt)\n",
    "        df = pd.read_csv(f\"../Data/Datasets/TrainTest/{vers}-{dset}-{tt}Set.csv\")\n",
    "        df = cleanData(df, 8, 0.01)\n",
    "        \n",
    "        print(\"\\n\", dset)\n",
    "        print(\"Initial shape:\", df.shape)\n",
    "        df.reset_index(drop=True, inplace=True) #Must reset index or it will not work\n",
    "        data = df.iloc[:, 7:] #Temperature is at index 7\n",
    "        data = pd.DataFrame(MinMaxScaler().fit_transform(data), columns = data.columns)\n",
    "\n",
    "        data = dropLowDistinction(data.iloc[:, 1:], tol)[0] #Do not drop temperature!\n",
    "        print(\"Shape after dropping low distinction:\", data.shape)\n",
    "\n",
    "        cols = data.columns\n",
    "        data = RobustScaler().fit_transform(data)\n",
    "        data = pd.DataFrame(data, columns = cols)\n",
    "        df = pd.concat([df.iloc[:, :7], data], axis=1)\n",
    "\n",
    "        df = removeOutliers(df, [dset], 3)\n",
    "        print(\"Shape after removing outliers:\", df.shape)\n",
    "\n",
    "        df.dropna(subset = df.columns[7:], inplace=True)\n",
    "        print(\"Shape after removing NaN:\", df.shape)\n",
    "\n",
    "        colDict[dset] = df.columns[7:] #Need to swap around targets so logS trains on logH and vice versa\n",
    "\n",
    "        df.to_csv(f\"../Data/Datasets/ScaledTrainTest/{vers}-{dset}-MRobust-{tt}Set.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
