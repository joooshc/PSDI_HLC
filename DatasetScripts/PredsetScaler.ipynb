{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Scaler\n",
    "\n",
    "Version: 0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, normalize\n",
    "from scipy import stats\n",
    "from MiscScripts.DataCleaning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.001\n",
    "vers = \"0.2.0\"\n",
    "datasets = [\"logHenry\", \"logS\"]\n",
    "\n",
    "datasetDict = {\"logHenry\" : \"hasLogH\",\n",
    "               \"logS\" : \"hasLogS\"}\n",
    "trainTest = [\"Train\", \"Validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " logHenry\n",
      "Train (9284, 165)\n",
      "Validation (2524, 165)\n",
      "Split index: 2524\n",
      "(11808, 165)\n",
      "\n",
      " Variable: logHenry\n",
      " (11808, 166)\n",
      "After dropping NaNs: (11808, 165)\n",
      "After dropping outliers: (11806, 165)\n",
      "After dropping NaNs again: (11806, 165)\n",
      "['HenryConstant-dataSource', 'nIsomers', 'MaxAbsEStateIndex', 'MaxEStateIndex']\n",
      "Initial shape: (11806, 165)\n",
      "After dropping low distinction: (11808, 80)\n",
      "(9284, 80) (2524, 80)\n",
      "\n",
      "\n",
      " logS\n",
      "Train (9179, 166)\n",
      "Validation (2524, 166)\n",
      "Split index: 2524\n",
      "(11703, 166)\n",
      "\n",
      " Variable: logS\n",
      " (11703, 167)\n",
      "After dropping NaNs: (11703, 166)\n",
      "After dropping outliers: (11670, 166)\n",
      "After dropping NaNs again: (11670, 166)\n",
      "['HenryConstant-dataSource', 'nIsomers', 'MaxAbsEStateIndex', 'MaxEStateIndex']\n",
      "Initial shape: (11670, 166)\n",
      "After dropping low distinction: (11703, 83)\n",
      "(9179, 83) (2524, 83)\n"
     ]
    }
   ],
   "source": [
    "for dset in datasets:\n",
    "    print(\"\\n\\n\", dset)\n",
    "    master = pd.DataFrame()\n",
    "\n",
    "    for tt in trainTest:\n",
    "        df = pd.read_csv(f\"../Data/Datasets/PredictionDatasets/0.2.0-{dset}-{tt}Set.csv\")\n",
    "        print(tt, df.shape)\n",
    "        splitIndex = df.shape[0]\n",
    "        master = pd.concat([master, df], axis=0)\n",
    "\n",
    "    print(\"Split index:\", splitIndex)\n",
    "    print(master.shape)\n",
    "    master.reset_index(drop=True, inplace=True)\n",
    "    oTarg = datasets[1-datasets.index(dset)]\n",
    "    oTargData = master[oTarg].values\n",
    "    \n",
    "    master.insert(10, f\"{oTarg}-Copy\", oTargData)\n",
    "\n",
    "    m = prepForScaling(master, dset, [])\n",
    "    print(master.columns.tolist()[6:10])\n",
    "    print(\"Initial shape:\", m.shape)\n",
    "    scaled = pd.DataFrame(MinMaxScaler().fit_transform(m.iloc[:, 8:]))\n",
    "\n",
    "    scaled, dropped = dropLowDistinction(pd.DataFrame(scaled), .005)\n",
    "    scaled = RobustScaler().fit_transform(scaled)\n",
    "    m = pd.concat([m.iloc[:, :8], pd.DataFrame(scaled)], axis=1)\n",
    "    print(\"After dropping low distinction:\", m.shape)\n",
    "\n",
    "    trainIndex = m.shape[0] - splitIndex\n",
    "    train = m.iloc[:trainIndex, :]; val = m.iloc[trainIndex:, :]\n",
    "    print(train.shape, val.shape)\n",
    "\n",
    "    train = removeOutliers(train, [dset], 3)\n",
    "\n",
    "    train.to_csv(f\"../Data/Datasets/ScaledPredictionDatasets/{vers}-{dset}-MRobust-TrainSet.csv\", index=False)\n",
    "    val.to_csv(f\"../Data/Datasets/ScaledPredictionDatasets/{vers}-{dset}-MRobust-ValidationSet.csv\", index=False)\n",
    "\n",
    "    train.to_csv(f\"../Data/Datasets/ScaledPredictionDatasets/{vers}-{oTarg}-MRobust-PredictionSet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logHlogS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = \"logHlogS\"\n",
    "# for tt in trainTest:\n",
    "#     df = pd.read_csv(f\"../Data/Datasets/TrainTest/{vers}-{dset}-{tt}Set.csv\")\n",
    "#     popped = df.pop(\"logS\")\n",
    "#     popped = MinMaxScaler().fit_transform(popped.values.reshape(-1, 1))\n",
    "#     df.insert(7, \"logS\", popped)\n",
    "\n",
    "#     df = prepForScaling(df, \"logHenry\", [])\n",
    "#     scaled = ScalingMethods.MRobustScaler(df.iloc[:, 8:], tol)\n",
    "#     df = pd.concat([df.iloc[:, :8], pd.DataFrame(scaled)], axis=1)\n",
    "\n",
    "#     print(df.columns.tolist()[6:10])\n",
    "#     df.to_csv(f\"../Data/Datasets/ScaledTrainTest/{vers}-{dset}-MRobust-{tt}Set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = \"logSlogH\"\n",
    "\n",
    "# for tt in trainTest:\n",
    "#     df = pd.read_csv(f\"../Data/Datasets/TrainTest/{vers}-{dset}-{tt}Set.csv\")\n",
    "#     popped = df.pop(\"logHenry\")\n",
    "#     popped = MinMaxScaler().fit_transform(popped.values.reshape(-1, 1))\n",
    "#     df.insert(7, \"logHenry\", popped)\n",
    "\n",
    "#     df = prepForScaling(df, \"logS\", [])\n",
    "#     scaled = ScalingMethods.MRobustScaler(df.iloc[:, 8:], tol)\n",
    "#     df = pd.concat([df.iloc[:, :8], pd.DataFrame(scaled)], axis=1)\n",
    "\n",
    "#     print(df.columns.tolist()[6:10])\n",
    "#     df.to_csv(f\"../Data/Datasets/ScaledTrainTest/{vers}-{dset}-MRobust-{tt}Set.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
